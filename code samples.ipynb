{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pypm.data_io import load_eod_data, load_spy_data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from typing import Dict, Any, Callable\n",
    "\n",
    "def calculate_return_series(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the return series of a given time series.\n",
    "    >>> data = load_eod_data('VBB')\n",
    "    >>> close_series = data['close']\n",
    "    >>> return_series = return_series(close_series)\n",
    "    The first value will always be NaN.\n",
    "    \"\"\"\n",
    "\n",
    "    shifted_series = series.shift(1, axis=0)\n",
    "    return series / shifted_series - 1\n",
    "\n",
    "\n",
    "def calculate_log_return_series(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Same as calculate_return_series but with log returns\n",
    "    \"\"\"\n",
    "    shifted_series = series.shift(1, axis=0)\n",
    "    return pd.Series(np.log(series / shifted_series))\n",
    "\n",
    "\n",
    "def calculate_percent_return(series: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Takes the first and last value in a series to determine the percent return, \n",
    "    assuming the series is in date-ascending order\n",
    "    \"\"\"\n",
    "    return series.iloc[-1] / series.iloc[0] - 1\n",
    "\n",
    "\n",
    "def get_years_past(series: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the years past according to the index of the series for use with\n",
    "    functions that require annualization   \n",
    "    \"\"\"\n",
    "    start_date = series.index[0]\n",
    "    end_date = series.index[-1]\n",
    "    return (end_date - start_date).days / 365.25\n",
    "\n",
    "\n",
    "def calculate_cagr(series: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate compounded annual growth rate\n",
    "    \"\"\"\n",
    "    start_price = series.iloc[0]\n",
    "    end_price = series.iloc[-1]\n",
    "    value_factor = end_price / start_price\n",
    "    year_past = get_years_past(series)\n",
    "    return (value_factor ** (1 / year_past)) - 1\n",
    "\n",
    "\n",
    "def calculate_annualized_volatility(return_series: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculates annualized volatility for a date-indexed return series. \n",
    "    Works for any interval of date-indexed prices and returns.\n",
    "    \"\"\"\n",
    "    years_past = get_years_past(return_series)\n",
    "    entries_per_year = return_series.shape[0] / years_past\n",
    "    return return_series.std() * np.sqrt(entries_per_year)\n",
    "\n",
    "\n",
    "def calculate_sharpe_ratio(price_series: pd.Series, \n",
    "    benchmark_rate: float=0) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the sharpe ratio given a price series. Defaults to benchmark_rate\n",
    "    of zero.\n",
    "    \"\"\"\n",
    "    cagr = calculate_cagr(price_series)\n",
    "    return_series = calculate_return_series(price_series)\n",
    "    volatility = calculate_annualized_volatility(return_series)\n",
    "    return (cagr - benchmark_rate) / volatility\n",
    "\n",
    "\n",
    "def calculate_rolling_sharpe_ratio(price_series: pd.Series,\n",
    "    n: float=20) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute an approximation of the sharpe ratio on a rolling basis. \n",
    "    Intended for use as a preference value.\n",
    "    \"\"\"\n",
    "    rolling_return_series = calculate_return_series(price_series).rolling(n)\n",
    "    return rolling_return_series.mean() / rolling_return_series.std()\n",
    "\n",
    "\n",
    "def calculate_annualized_downside_deviation(return_series: pd.Series,\n",
    "    benchmark_rate: float=0) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the downside deviation for use in the sortino ratio.\n",
    "    Benchmark rate is assumed to be annualized. It will be adjusted according\n",
    "    to the number of periods per year seen in the data.\n",
    "    \"\"\"\n",
    "\n",
    "    # For both de-annualizing the benchmark rate and annualizing result\n",
    "    years_past = get_years_past(return_series)\n",
    "    entries_per_year = return_series.shape[0] / years_past\n",
    "\n",
    "    adjusted_benchmark_rate = ((1+benchmark_rate) ** (1/entries_per_year)) - 1\n",
    "\n",
    "    downside_series = adjusted_benchmark_rate - return_series\n",
    "    downside_sum_of_squares = (downside_series[downside_series > 0] ** 2).sum()\n",
    "    denominator = return_series.shape[0] - 1\n",
    "    downside_deviation = np.sqrt(downside_sum_of_squares / denominator)\n",
    "\n",
    "    return downside_deviation * np.sqrt(entries_per_year)\n",
    "\n",
    "\n",
    "def calculate_sortino_ratio(price_series: pd.Series,\n",
    "    benchmark_rate: float=0) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the sortino ratio.\n",
    "    \"\"\"\n",
    "    cagr = calculate_cagr(price_series)\n",
    "    return_series = calculate_return_series(price_series)\n",
    "    downside_deviation = calculate_annualized_downside_deviation(return_series)\n",
    "    return (cagr - benchmark_rate) / downside_deviation\n",
    "\n",
    "\n",
    "def calculate_pure_profit_score(price_series: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the pure profit score\n",
    "    \"\"\"\n",
    "    cagr = calculate_cagr(price_series)\n",
    "\n",
    "    # Build a single column for a predictor, t\n",
    "    t: np.ndarray = np.arange(0, price_series.shape[0]).reshape(-1, 1)\n",
    "\n",
    "    # Fit the regression\n",
    "    regression = LinearRegression().fit(t, price_series)\n",
    "\n",
    "    # Get the r-squared value\n",
    "    r_squared = regression.score(t, price_series)\n",
    "\n",
    "    return cagr * r_squared\n",
    "\n",
    "def calculate_jensens_alpha(return_series: pd.Series, \n",
    "    benchmark_return_series: pd.Series) -> float: \n",
    "    \"\"\"\n",
    "    Calculates jensens alpha. Prefers input series have the same index. Handles\n",
    "    NAs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Join series along date index and purge NAs\n",
    "    df = pd.concat([return_series, benchmark_return_series], sort=True, axis=1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Get the appropriate data structure for scikit learn\n",
    "    clean_returns: pd.Series = df[df.columns.values[0]]\n",
    "    clean_benchmarks = pd.DataFrame(df[df.columns.values[1]])\n",
    "\n",
    "    # Fit a linear regression and return the alpha\n",
    "    regression = LinearRegression().fit(clean_benchmarks, y=clean_returns)\n",
    "    return regression.intercept_\n",
    "\n",
    "def calculate_jensens_alpha_v2(return_series: pd.Series) -> float: \n",
    "    \"\"\"\n",
    "    Calculates jensens alpha, but loads in SPY prices as the benchmark series \n",
    "    for you. Can be slow if run repeatedly.\n",
    "    \"\"\"\n",
    "    spy_data = load_spy_data()\n",
    "    benchmark_return_series = calculate_log_return_series(spy_data['close'])\n",
    "    return calculate_jensens_alpha(return_series, benchmark_return_series)\n",
    "    \n",
    "\n",
    "DRAWDOWN_EVALUATORS: Dict[str, Callable] = {\n",
    "    'dollar': lambda price, peak: peak - price,\n",
    "    'percent': lambda price, peak: -((price / peak) - 1),\n",
    "    'log': lambda price, peak: np.log(peak) - np.log(price),\n",
    "}\n",
    "\n",
    "def calculate_drawdown_series(series: pd.Series, method: str='log') -> pd.Series:\n",
    "    \"\"\"\n",
    "    Returns the drawdown series\n",
    "    \"\"\"\n",
    "    assert method in DRAWDOWN_EVALUATORS, \\\n",
    "        f'Method \"{method}\" must by one of {list(DRAWDOWN_EVALUATORS.keys())}'\n",
    "\n",
    "    evaluator = DRAWDOWN_EVALUATORS[method]\n",
    "    return evaluator(series, series.cummax())\n",
    "\n",
    "def calculate_max_drawdown(series: pd.Series, method: str='log') -> float:\n",
    "    \"\"\"\n",
    "    Simply returns the max drawdown as a float\n",
    "    \"\"\"\n",
    "    return calculate_drawdown_series(series, method).max()\n",
    "\n",
    "def calculate_max_drawdown_with_metadata(series: pd.Series, \n",
    "    method: str='log') -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculates max_drawndown and stores metadata about when and where. Returns \n",
    "    a dictionary of the form \n",
    "        {\n",
    "            'max_drawdown': float,\n",
    "            'peak_date': pd.Timestamp,\n",
    "            'peak_price': float,\n",
    "            'trough_date': pd.Timestamp,\n",
    "            'trough_price': float,\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    assert method in DRAWDOWN_EVALUATORS, \\\n",
    "        f'Method \"{method}\" must by one of {list(DRAWDOWN_EVALUATORS.keys())}'\n",
    "\n",
    "    evaluator = DRAWDOWN_EVALUATORS[method]\n",
    "\n",
    "    max_drawdown = 0\n",
    "    local_peak_date = peak_date = trough_date = series.index[0]\n",
    "    local_peak_price = peak_price = trough_price = series.iloc[0]\n",
    "\n",
    "    for date, price in series.iteritems():\n",
    "\n",
    "        # Keep track of the rolling max\n",
    "        if price > local_peak_price:\n",
    "            local_peak_date = date\n",
    "            local_peak_price = price\n",
    "\n",
    "        # Compute the drawdown\n",
    "        drawdown = evaluator(price, local_peak_price)\n",
    "\n",
    "        # Store new max drawdown values\n",
    "        if drawdown > max_drawdown:\n",
    "            max_drawdown = drawdown\n",
    "\n",
    "            peak_date = local_peak_date\n",
    "            peak_price = local_peak_price\n",
    "\n",
    "            trough_date = date\n",
    "            trough_price = price\n",
    "\n",
    "    return {\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'peak_date': peak_date,\n",
    "        'peak_price': peak_price,\n",
    "        'trough_date': trough_date,\n",
    "        'trough_price': trough_price\n",
    "    }\n",
    "\n",
    "def calculate_log_max_drawdown_ratio(series: pd.Series) -> float:\n",
    "    log_drawdown = calculate_max_drawdown(series, method='log')\n",
    "    log_return = np.log(series.iloc[-1]) - np.log(series.iloc[0])\n",
    "    return log_return - log_drawdown\n",
    "\n",
    "def calculate_calmar_ratio(series: pd.Series, years_past: int=3) -> float:\n",
    "    \"\"\"\n",
    "    Return the percent max drawdown ratio over the past three years, otherwise \n",
    "    known as the Calmar Ratio\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter series on past three years\n",
    "    last_date = series.index[-1]\n",
    "    three_years_ago = last_date - pd.Timedelta(days=years_past*365.25)\n",
    "    series = series[series.index > three_years_ago]\n",
    "\n",
    "    # Compute annualized percent max drawdown ratio\n",
    "    percent_drawdown = calculate_max_drawdown(series, method='percent')\n",
    "    cagr = calculate_cagr(series)\n",
    "    return cagr / percent_drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indicators\n",
    "\n",
    "import pandas as pd\n",
    "from pypm.data_io import load_eod_data\n",
    "\n",
    "\n",
    "def calculate_simple_moving_average(series: pd.Series, n: int=20) -> pd.Series:\n",
    "    \"\"\"Calculates the simple moving average\"\"\"\n",
    "    return series.rolling(n).mean()\n",
    "\n",
    "\n",
    "def calculate_simple_moving_sample_stdev(series: pd.Series, n: int=20) -> pd.Series:\n",
    "    \"\"\"Calculates the simple moving average\"\"\"\n",
    "    return series.rolling(n).std()\n",
    "\n",
    "\n",
    "def calculate_macd_oscillator(series: pd.Series,\n",
    "    n1: int=5, n2: int=34) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the moving average convergence divergence oscillator, given a \n",
    "    short moving average of length n1 and a long moving average of length n2\n",
    "    \"\"\"\n",
    "    assert n1 < n2, f'n1 must be less than n2'\n",
    "    return calculate_simple_moving_average(series, n1) - \\\n",
    "        calculate_simple_moving_average(series, n2)\n",
    "\n",
    "\n",
    "def calculate_bollinger_bands(series: pd.Series, n: int=20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the bollinger bands and returns them as a dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    sma = calculate_simple_moving_average(series, n)\n",
    "    stdev = calculate_simple_moving_sample_stdev(series, n)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'middle': sma,\n",
    "        'upper': sma + 2 * stdev,\n",
    "        'lower': sma - 2 * stdev\n",
    "    })\n",
    "\n",
    "\n",
    "def calculate_money_flow_volume_series(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates money flow series\n",
    "    \"\"\"\n",
    "    mfv = df['volume'] * (2*df['close'] - df['high'] - df['low']) / \\\n",
    "                                    (df['high'] - df['low'])\n",
    "    return mfv\n",
    "\n",
    "def calculate_money_flow_volume(df: pd.DataFrame, n: int=20) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates money flow volume, or q_t in our formula\n",
    "    \"\"\"\n",
    "    return calculate_money_flow_volume_series(df).rolling(n).sum()\n",
    "\n",
    "def calculate_chaikin_money_flow(df: pd.DataFrame, n: int=20) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the Chaikin money flow\n",
    "    \"\"\"\n",
    "    return calculate_money_flow_volume(df, n) / df['volume'].rolling(n).sum()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = load_eod_data('AWU')\n",
    "    closes = data['close']\n",
    "    sma = calculate_simple_moving_average(closes, 10)\n",
    "    macd = calculate_macd_oscillator(closes, 5, 50)\n",
    "\n",
    "    bollinger_bands = calculate_bollinger_bands(closes, 100)\n",
    "    bollinger_bands = bollinger_bands.assign(closes=closes)\n",
    "    bollinger_bands.plot()\n",
    "\n",
    "    cmf = calculate_chaikin_money_flow(data)\n",
    "    # cmf.plot()\n",
    "\n",
    "  \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# singals\n",
    "\n",
    "from pypm import metrics, signals, data_io, simulation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "from itertools import product\n",
    "from timeit import default_timer\n",
    "from typing import Dict, Tuple, List, Callable, Iterable, Any, NewType, Mapping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm \n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "\n",
    "# Performance data and parameter inputs are dictionaries\n",
    "Parameters = NewType('Parameters', Dict[str, float])\n",
    "Performance = simulation.PortfolioHistory.PerformancePayload # Dict[str, float]\n",
    "\n",
    "# Simulation function must take parameters as keyword arguments pointing to \n",
    "# iterables and return a performance metric dictionary\n",
    "SimKwargs = NewType('Kwargs', Mapping[str, Iterable[Any]])\n",
    "SimFunction = NewType('SimFunction', Callable[[SimKwargs], Performance])\n",
    "\n",
    "class OptimizationResult(object):\n",
    "    \"\"\"Simple container class for optimization data\"\"\"\n",
    "\n",
    "    def __init__(self, parameters: Parameters, performance: Performance):\n",
    "\n",
    "        # Make sure no collisions between performance metrics and params\n",
    "        assert len(parameters.keys() & performance.keys()) == 0, \\\n",
    "            'parameter name matches performance metric name'\n",
    "\n",
    "        self.parameters = parameters\n",
    "        self.performance = performance\n",
    "\n",
    "    @property\n",
    "    def as_dict(self) -> Dict[str, float]:\n",
    "        \"\"\"Combines the dictionaries after we are sure of no collisions\"\"\"\n",
    "        return {**self.parameters, **self.performance}\n",
    "    \n",
    "\n",
    "class GridSearchOptimizer(object):\n",
    "    \"\"\"\n",
    "    A generic grid search optimizer that requires only a simulation function and\n",
    "    a series of parameter ranges. Provides timing, summary, and plotting \n",
    "    utilities with return data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, simulation_function: SimFunction):\n",
    "\n",
    "        self.simulate = simulation_function\n",
    "        self._results_list: List[OptimizationResult] = list()\n",
    "        self._results_df = pd.DataFrame()\n",
    "\n",
    "        self._optimization_finished = False\n",
    "\n",
    "    def add_results(self, parameters: Parameters, performance: Performance):\n",
    "        _results = OptimizationResult(parameters, performance)\n",
    "        self._results_list.append(_results)\n",
    "\n",
    "    def optimize(self, **optimization_ranges: SimKwargs):\n",
    "\n",
    "        assert optimization_ranges, 'Must provide non-empty parameters.'\n",
    "\n",
    "        # Convert all iterables to lists\n",
    "        param_ranges = {k: list(v) for k, v in optimization_ranges.items()}\n",
    "        self.param_names = param_names = list(param_ranges.keys())\n",
    "\n",
    "        # Count total simulation\n",
    "        n = total_simulations = np.prod([len(r) for r in param_ranges.values()])\n",
    "\n",
    "        total_time_elapsed = 0\n",
    "\n",
    "        print(f'Starting simulation ...')\n",
    "        print(f'Simulating 1 / {n} ...', end='\\r')\n",
    "        for i, params in enumerate(product(*param_ranges.values())):\n",
    "            if i > 0:\n",
    "                _avg = avg_time = total_time_elapsed / i\n",
    "                _rem = remaining_time = (n - (i + 1)) * avg_time\n",
    "                s =  f'Simulating {i+1} / {n} ... '\n",
    "                s += f'{_rem:.0f}s remaining ({_avg:.1f}s avg)'\n",
    "                s += ' '*8\n",
    "                print(s, end='\\r')\n",
    "\n",
    "            timer_start = default_timer()\n",
    "\n",
    "            parameters = {n: param for n, param in zip(param_names, params)}\n",
    "            results = self.simulate(**parameters)\n",
    "            self.add_results(parameters, results)\n",
    "\n",
    "            timer_end = default_timer()\n",
    "            total_time_elapsed += timer_end - timer_start \n",
    "\n",
    "        print(f'Simulated {total_simulations} / {total_simulations} ...')\n",
    "        print(f'Elapsed time: {total_time_elapsed:.0f}s')\n",
    "        print(f'Done.')\n",
    "\n",
    "        self._optimization_finished = True\n",
    "\n",
    "    def _assert_finished(self):\n",
    "        assert self._optimization_finished, \\\n",
    "            'Run self.optimize before accessing this method.'\n",
    "\n",
    "    @property\n",
    "    def results(self) -> pd.DataFrame:\n",
    "        self._assert_finished()\n",
    "        if self._results_df.empty:\n",
    "\n",
    "            _results_list = self._results_list\n",
    "            self._results_df = pd.DataFrame([r.as_dict for r in _results_list])\n",
    "\n",
    "            _columns = set(list(self._results_df.columns.values))\n",
    "            _params = set(self.param_names)\n",
    "            self.metric_names = list(_columns - _params)\n",
    "\n",
    "        return self._results_df\n",
    "\n",
    "    def print_summary(self):\n",
    "        df = self.results\n",
    "        metric_names = self.metric_names\n",
    "\n",
    "        print('Summary statistics')\n",
    "        print(df[metric_names].describe().T)\n",
    "\n",
    "    def get_best(self, metric_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Sort the results by a specific performance metric\n",
    "        \"\"\"\n",
    "        self._assert_finished()\n",
    "\n",
    "        results = self.results\n",
    "        param_names = self.param_names\n",
    "        metric_names = self.metric_names\n",
    "\n",
    "        assert metric_name in metric_names, 'Not a performance metric'\n",
    "        partial_df = self.results[param_names+[metric_name]]\n",
    "\n",
    "        return partial_df.sort_values(metric_name, ascending=False)\n",
    "\n",
    "    def plot_1d_hist(self, x, show=True):\n",
    "        self.results.hist(x)\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def plot_2d_line(self, x, y, show=True, **filter_kwargs):\n",
    "        _results = self.results\n",
    "        for k, v in filter_kwargs.items():\n",
    "            _results = _results[getattr(_results, k) == v]\n",
    "\n",
    "        ax = _results.plot(x, y)\n",
    "        if filter_kwargs:\n",
    "            k_str = ', '.join([f'{k}={v}' for k,v in filter_kwargs.items()])\n",
    "            ax.legend([f'{x} ({k_str})'])\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def plot_2d_violin(self, x, y, show=True):\n",
    "        \"\"\"\n",
    "        Group y along x then plot violin charts\n",
    "        \"\"\"\n",
    "        x_values = self.results[x].unique()\n",
    "        x_values.sort()\n",
    "\n",
    "        y_by_x = OrderedDict([(v, []) for v in x_values])\n",
    "        for _, row in self.results.iterrows():\n",
    "            y_by_x[row[x]].append(row[y])\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ax.violinplot(dataset=list(y_by_x.values()), showmedians=True)\n",
    "        ax.set_xlabel(x)\n",
    "        ax.set_ylabel(y)\n",
    "        ax.set_xticks(range(0, len(y_by_x)+1))\n",
    "        ax.set_xticklabels([''] + list(y_by_x.keys()))\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def plot_3d_mesh(self, x, y, z, show=True, **filter_kwargs):\n",
    "        \"\"\"\n",
    "        Plot interactive 3d mesh. z axis should typically be performance metric\n",
    "        \"\"\"\n",
    "        _results = self.results\n",
    "        fig = plt.figure()\n",
    "        ax = Axes3D(fig)\n",
    "\n",
    "        for k, v in filter_kwargs.items():\n",
    "            _results = _results[getattr(_results, k) == v]\n",
    "\n",
    "        X, Y, Z = [getattr(_results, attr) for attr in (x, y, z)]\n",
    "        ax.plot_trisurf(X, Y, Z, cmap=cm.jet, linewidth=0.2)\n",
    "        ax.set_xlabel(x)\n",
    "        ax.set_ylabel(y)\n",
    "        ax.set_zlabel(z)\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def plot(self, *attrs: Tuple[str], show=True, \n",
    "        **filter_kwargs: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Attempt to intelligently dispatch plotting functions based on the number\n",
    "        and type of attributes. Last argument should typically be the \n",
    "        performance metric.\n",
    "        \"\"\"\n",
    "        self._assert_finished()\n",
    "        param_names = self.param_names\n",
    "        metric_names = self.metric_names\n",
    "\n",
    "        if len(attrs) == 3:\n",
    "            assert attrs[0] in param_names and attrs[1] in param_names, \\\n",
    "                'First two positional arguments must be parameter names.'\n",
    "\n",
    "            assert attrs[2] in metric_names, \\\n",
    "                'Last positional argument must be a metric name.'\n",
    "\n",
    "            assert len(filter_kwargs) + 2 == len(param_names), \\\n",
    "                'Must filter remaining parameters. e.g. p_three=some_number.'\n",
    "\n",
    "            self.plot_3d_mesh(*attrs, show=show, **filter_kwargs)\n",
    "\n",
    "        elif len(attrs) == 2:\n",
    "            if len(param_names) == 1 or filter_kwargs:\n",
    "                self.plot_2d_line(*attrs, show=show, **filter_kwargs)\n",
    "\n",
    "            elif len(param_names) > 1:\n",
    "                self.plot_2d_violin(*attrs, show=show)\n",
    "\n",
    "        elif len(attrs) == 1:\n",
    "            self.plot_1d_hist(*attrs, show=show)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Must pass between one and three column names.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#portfolio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple, List, Dict, Callable, NewType, Any\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from pypm import metrics, signals, data_io\n",
    "\n",
    "Symbol = NewType('Symbol', str)\n",
    "Dollars = NewType('Dollars', float)\n",
    "\n",
    "DATE_FORMAT_STR = '%a %b %d, %Y'\n",
    "def _pdate(date: pd.Timestamp):\n",
    "    \"\"\"Pretty-print a datetime with just the date\"\"\"\n",
    "    return date.strftime(DATE_FORMAT_STR)\n",
    "\n",
    "class Position(object):\n",
    "    \"\"\"\n",
    "    A simple object to hold and manipulate data related to long stock trades.\n",
    "    Allows a single buy and sell operation on an asset for a constant number of \n",
    "    shares.\n",
    "    The __init__ method is equivelant to a buy operation. The exit\n",
    "    method is a sell operation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, symbol: Symbol, entry_date: pd.Timestamp, \n",
    "        entry_price: Dollars, shares: int):\n",
    "        \"\"\"\n",
    "        Equivelent to buying a certain number of shares of the asset\n",
    "        \"\"\"\n",
    "\n",
    "        # Recorded on initialization\n",
    "        self.entry_date = entry_date\n",
    "\n",
    "        assert entry_price > 0, 'Cannot buy asset with zero or negative price.'\n",
    "        self.entry_price = entry_price\n",
    "\n",
    "        assert shares > 0, 'Cannot buy zero or negative shares.'\n",
    "        self.shares = shares\n",
    "\n",
    "        self.symbol = symbol\n",
    "\n",
    "        # Recorded on position exit\n",
    "        self.exit_date: pd.Timestamp = None\n",
    "        self.exit_price: Dollars = None\n",
    "\n",
    "        # For easily getting current portolio value\n",
    "        self.last_date: pd.Timestamp = None\n",
    "        self.last_price: Dollars = None\n",
    "\n",
    "        # Updated intermediately\n",
    "        self._dict_series: Dict[pd.Timestamp, Dollars] = OrderedDict()\n",
    "        self.record_price_update(entry_date, entry_price)\n",
    "\n",
    "        # Cache control for pd.Series representation\n",
    "        self._price_series: pd.Series = None\n",
    "        self._needs_update_pd_series: bool = True\n",
    "\n",
    "    def exit(self, exit_date, exit_price):\n",
    "        \"\"\"\n",
    "        Equivelent to selling a stock holding\n",
    "        \"\"\"\n",
    "        assert self.entry_date != exit_date, 'Churned a position same-day.'\n",
    "        assert not self.exit_date, 'Position already closed.'\n",
    "        self.record_price_update(exit_date, exit_price)\n",
    "        self.exit_date = exit_date\n",
    "        self.exit_price = exit_price\n",
    "\n",
    "    def record_price_update(self, date, price):\n",
    "        \"\"\"\n",
    "        Stateless function to record intermediate prices of existing positions\n",
    "        \"\"\"\n",
    "        self.last_date = date\n",
    "        self.last_price = price\n",
    "        self._dict_series[date] = price\n",
    "\n",
    "        # Invalidate cache on self.price_series\n",
    "        self._needs_update_pd_series = True\n",
    "\n",
    "    @property\n",
    "    def price_series(self) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Returns cached readonly pd.Series \n",
    "        \"\"\"\n",
    "        if self._needs_update_pd_series or self._price_series is None:\n",
    "            self._price_series = pd.Series(self._dict_series)\n",
    "            self._needs_update_pd_series = False\n",
    "        return self._price_series\n",
    "\n",
    "    @property\n",
    "    def last_value(self) -> Dollars:\n",
    "        return self.last_price * self.shares\n",
    "\n",
    "    @property\n",
    "    def is_active(self) -> bool:\n",
    "        return self.exit_date is None\n",
    "\n",
    "    @property\n",
    "    def is_closed(self) -> bool:\n",
    "        return not self.is_active\n",
    "    \n",
    "    @property\n",
    "    def value_series(self) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Returns the value of the position over time. Ignores self.exit_date.\n",
    "        Used in calculating the equity curve.\n",
    "        \"\"\"\n",
    "        assert self.is_closed, 'Position must be closed to access this property'\n",
    "        return self.shares * self.price_series[:-1]\n",
    "\n",
    "    @property\n",
    "    def percent_return(self) -> float:\n",
    "        return (self.exit_price / self.entry_price) - 1\n",
    "    \n",
    "    @property\n",
    "    def entry_value(self) -> Dollars:\n",
    "        return self.shares * self.entry_price\n",
    "\n",
    "    @property\n",
    "    def exit_value(self) -> Dollars:\n",
    "        return self.shares * self.exit_price\n",
    "\n",
    "    @property\n",
    "    def change_in_value(self) -> Dollars:\n",
    "        return self.exit_value - self.entry_value\n",
    "\n",
    "    @property\n",
    "    def trade_length(self):\n",
    "        return len(self._dict_series) - 1\n",
    "    \n",
    "    def print_position_summary(self):\n",
    "        _entry_date = _pdate(self.entry_date)\n",
    "        _exit_date = _pdate(self.exit_date)\n",
    "        _days = self.trade_length\n",
    "\n",
    "        _entry_price = round(self.entry_price, 2)\n",
    "        _exit_price = round(self.exit_price, 2)\n",
    "\n",
    "        _entry_value = round(self.entry_value, 2)\n",
    "        _exit_value = round(self.exit_value, 2)\n",
    "\n",
    "        _return = round(100 * self.percent_return, 1)\n",
    "        _diff = round(self.change_in_value, 2)\n",
    "\n",
    "        print(f'{self.symbol:<5}     Trade summary')\n",
    "        print(f'Date:     {_entry_date} -> {_exit_date} [{_days} days]')\n",
    "        print(f'Price:    ${_entry_price} -> ${_exit_price} [{_return}%]')\n",
    "        print(f'Value:    ${_entry_value} -> ${_exit_value} [${_diff}]')\n",
    "        print()\n",
    "\n",
    "    def __hash__(self):\n",
    "        \"\"\"\n",
    "        A unique position will be defined by a unique combination of an \n",
    "        entry_date and symbol, in accordance with our constraints regarding \n",
    "        duplicate, variable, and compound positions\n",
    "        \"\"\"\n",
    "        return hash((self.entry_date, self.symbol))\n",
    "\n",
    "\n",
    "class PortfolioHistory(object):\n",
    "    \"\"\"\n",
    "    Holds Position objects and keeps track of portfolio variables.\n",
    "    Produces summary statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Keep track of positions, recorded in this list after close\n",
    "        self.position_history: List[Position] = []\n",
    "        self._logged_positions: Set[Position] = set()\n",
    "\n",
    "        # Keep track of the last seen date\n",
    "        self.last_date: pd.Timestamp = pd.Timestamp.min\n",
    "\n",
    "        # Readonly fields\n",
    "        self._cash_history: Dict[pd.Timestamp, Dollars] = dict()\n",
    "        self._simulation_finished = False\n",
    "        self._spy: pd.DataFrame = pd.DataFrame()\n",
    "        self._spy_log_returns: pd.Series = pd.Series()\n",
    "\n",
    "    def add_to_history(self, position: Position):\n",
    "        _log = self._logged_positions\n",
    "        assert not position in _log, 'Recorded the same position twice.'\n",
    "        assert position.is_closed, 'Position is not closed.'\n",
    "        self._logged_positions.add(position)\n",
    "        self.position_history.append(position)\n",
    "        self.last_date = max(self.last_date, position.last_date)\n",
    "\n",
    "    def record_cash(self, date, cash):\n",
    "        self._cash_history[date] = cash\n",
    "        self.last_date = max(self.last_date, date)\n",
    "\n",
    "    @staticmethod\n",
    "    def _as_oseries(d: Dict[pd.Timestamp, Any]) -> pd.Series:\n",
    "        return pd.Series(d).sort_index()\n",
    "\n",
    "    def _compute_cash_series(self):\n",
    "        self._cash_series = self._as_oseries(self._cash_history)\n",
    "\n",
    "    @property\n",
    "    def cash_series(self) -> pd.Series:\n",
    "        return self._cash_series\n",
    "\n",
    "    def _compute_portfolio_value_series(self):\n",
    "        value_by_date = defaultdict(float)\n",
    "        last_date = self.last_date\n",
    "\n",
    "        # Add up value of assets\n",
    "        for position in self.position_history:\n",
    "            for date, value in position.value_series.items():\n",
    "                value_by_date[date] += value\n",
    "\n",
    "        # Make sure all dates in cash_series are present\n",
    "        for date in self.cash_series.index:\n",
    "            value_by_date[date] += 0\n",
    "\n",
    "        self._portfolio_value_series = self._as_oseries(value_by_date)\n",
    "\n",
    "    @property\n",
    "    def portfolio_value_series(self):\n",
    "        return self._portfolio_value_series\n",
    "\n",
    "    def _compute_equity_series(self):\n",
    "        c_series = self.cash_series\n",
    "        p_series = self.portfolio_value_series\n",
    "        assert all(c_series.index == p_series.index), \\\n",
    "            'portfolio_series has dates not in cash_series'\n",
    "        self._equity_series = c_series + p_series     \n",
    "\n",
    "    @property\n",
    "    def equity_series(self):\n",
    "        return self._equity_series\n",
    "\n",
    "    def _compute_log_return_series(self):\n",
    "        self._log_return_series = \\\n",
    "            metrics.calculate_log_return_series(self.equity_series)\n",
    "\n",
    "    @property\n",
    "    def log_return_series(self):\n",
    "        return self._log_return_series\n",
    "\n",
    "    def _assert_finished(self):\n",
    "        assert self._simulation_finished, \\\n",
    "            'Simuation must be finished by running self.finish() in order ' + \\\n",
    "            'to access this method or property.'\n",
    "\n",
    "    def finish(self):\n",
    "        \"\"\"\n",
    "        Notate that the simulation is finished and compute readonly values\n",
    "        \"\"\"\n",
    "        self._simulation_finished = True\n",
    "        self._compute_cash_series()\n",
    "        self._compute_portfolio_value_series()\n",
    "        self._compute_equity_series()\n",
    "        self._compute_log_return_series()\n",
    "        self._assert_finished()\n",
    "\n",
    "    def compute_portfolio_size_series(self) -> pd.Series:\n",
    "        size_by_date = defaultdict(int)\n",
    "        for position in self.position_history:\n",
    "            for date in position.value_series.index:\n",
    "                size_by_date[date] += 1\n",
    "        return self._as_oseries(size_by_date)\n",
    "\n",
    "    @property\n",
    "    def spy(self):\n",
    "        if self._spy.empty:\n",
    "            first_date = self.cash_series.index[0]\n",
    "            _spy = data_io.load_spy_data()\n",
    "            self._spy = _spy[_spy.index > first_date]\n",
    "        return self._spy\n",
    "\n",
    "    @property\n",
    "    def spy_log_returns(self):\n",
    "        if self._spy_log_returns.empty:\n",
    "            close = self.spy['close']\n",
    "            self._spy_log_returns =  metrics.calculate_log_return_series(close)\n",
    "        return self._spy_log_returns\n",
    "\n",
    "    @property\n",
    "    def percent_return(self):\n",
    "        return metrics.calculate_percent_return(self.equity_series)\n",
    "\n",
    "    @property\n",
    "    def spy_percent_return(self):\n",
    "        return metrics.calculate_percent_return(self.spy['close'])\n",
    "\n",
    "    @property\n",
    "    def cagr(self):\n",
    "        return metrics.calculate_cagr(self.equity_series)\n",
    "\n",
    "    @property\n",
    "    def volatility(self):\n",
    "        return metrics.calculate_annualized_volatility(self.log_return_series)\n",
    "\n",
    "    @property\n",
    "    def sharpe_ratio(self):\n",
    "        return metrics.calculate_sharpe_ratio(self.equity_series)\n",
    "\n",
    "    @property\n",
    "    def spy_cagr(self):\n",
    "        return metrics.calculate_cagr(self.spy['close'])\n",
    "    \n",
    "    @property\n",
    "    def excess_cagr(self):\n",
    "        return self.cagr - self.spy_cagr\n",
    "\n",
    "    @property\n",
    "    def jensens_alpha(self):\n",
    "        return metrics.calculate_jensens_alpha(\n",
    "            self.log_return_series,\n",
    "            self.spy_log_returns,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def dollar_max_drawdown(self):\n",
    "        return metrics.calculate_max_drawdown(self.equity_series, 'dollar')\n",
    "\n",
    "    @property\n",
    "    def percent_max_drawdown(self):\n",
    "        return metrics.calculate_max_drawdown(self.equity_series, 'percent')\n",
    "\n",
    "    @property\n",
    "    def log_max_drawdown_ratio(self):\n",
    "        return metrics.calculate_log_max_drawdown_ratio(self.equity_series)\n",
    "    \n",
    "    @property\n",
    "    def number_of_trades(self):\n",
    "        return len(self.position_history)\n",
    "\n",
    "    @property\n",
    "    def average_active_trades(self):\n",
    "        return self.compute_portfolio_size_series().mean()\n",
    "\n",
    "    @property\n",
    "    def final_cash(self):\n",
    "        self._assert_finished()\n",
    "        return self.cash_series[-1]\n",
    "    \n",
    "    @property\n",
    "    def final_equity(self):\n",
    "        self._assert_finished()\n",
    "        return self.equity_series[-1]\n",
    "    \n",
    "    _PERFORMANCE_METRICS_PROPS = [\n",
    "        'percent_return',\n",
    "        'spy_percent_return',\n",
    "        'cagr',\n",
    "        'volatility',\n",
    "        'sharpe_ratio',\n",
    "        'spy_cagr',\n",
    "        'excess_cagr',\n",
    "        'jensens_alpha',\n",
    "        'dollar_max_drawdown',\n",
    "        'percent_max_drawdown',\n",
    "        'log_max_drawdown_ratio',\n",
    "        'number_of_trades',\n",
    "        'average_active_trades',\n",
    "        'final_cash',\n",
    "        'final_equity',\n",
    "    ]\n",
    "\n",
    "    PerformancePayload = NewType('PerformancePayload', Dict[str, float])\n",
    "\n",
    "    def get_performance_metric_data(self) -> PerformancePayload:\n",
    "        props = self._PERFORMANCE_METRICS_PROPS\n",
    "        return {prop: getattr(self, prop) for prop in props}\n",
    "\n",
    "    def print_position_summaries(self):\n",
    "        for position in self.position_history:\n",
    "            position.print_position_summary()\n",
    "\n",
    "    def print_summary(self):\n",
    "        self._assert_finished()\n",
    "        s = f'Equity: ${self.final_equity:.2f}\\n' \\\n",
    "            f'Percent Return: {100*self.percent_return:.2f}%\\n' \\\n",
    "            f'S&P 500 Return: {100*self.spy_percent_return:.2f}%\\n\\n' \\\n",
    "            f'Number of trades: {self.number_of_trades}\\n' \\\n",
    "            f'Average active trades: {self.average_active_trades:.2f}\\n\\n' \\\n",
    "            f'CAGR: {100*self.cagr:.2f}%\\n' \\\n",
    "            f'S&P 500 CAGR: {100*self.spy_cagr:.2f}%\\n' \\\n",
    "            f'Excess CAGR: {100*self.excess_cagr:.2f}%\\n\\n' \\\n",
    "            f'Annualized Volatility: {100*self.volatility:.2f}%\\n' \\\n",
    "            f'Sharpe Ratio: {self.sharpe_ratio:.2f}\\n' \\\n",
    "            f'Jensen\\'s Alpha: {self.jensens_alpha:.6f}\\n\\n' \\\n",
    "            f'Dollar Max Drawdown: ${self.dollar_max_drawdown:.2f}\\n' \\\n",
    "            f'Percent Max Drawdown: {100*self.percent_max_drawdown:.2f}%\\n' \\\n",
    "            f'Log Max Drawdown Ratio: {self.log_max_drawdown_ratio:.2f}\\n'\n",
    "\n",
    "        print(s)\n",
    "\n",
    "    def plot(self, show=True) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Plots equity, cash and portfolio value curves.\n",
    "        \"\"\"\n",
    "        self._assert_finished()\n",
    "\n",
    "        figure, axes = plt.subplots(nrows=3, ncols=1)\n",
    "        figure.tight_layout(pad=3.0)\n",
    "        axes[0].plot(self.equity_series)\n",
    "        axes[0].set_title('Equity')\n",
    "        axes[0].grid()\n",
    "\n",
    "        axes[1].plot(self.cash_series)\n",
    "        axes[1].set_title('Cash')\n",
    "        axes[1].grid()\n",
    "\n",
    "        axes[2].plot(self.portfolio_value_series)\n",
    "        axes[2].set_title('Portfolio Value')\n",
    "        axes[2].grid()\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "        return figure\n",
    "\n",
    "    def plot_benchmark_comparison(self, show=True) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Plot comparable investment in the S&P 500.\n",
    "        \"\"\"\n",
    "        self._assert_finished()\n",
    "\n",
    "        equity_curve = self.equity_series\n",
    "        ax = equity_curve.plot()\n",
    "\n",
    "        spy_closes = self.spy['close']\n",
    "        initial_cash = self.cash_series[0]\n",
    "        initial_spy = spy_closes[0]\n",
    "\n",
    "        scaled_spy = spy_closes * (initial_cash / initial_spy)\n",
    "        scaled_spy.plot()\n",
    "\n",
    "        baseline = pd.Series(initial_cash, index=equity_curve.index)\n",
    "        ax = baseline.plot(color='black')\n",
    "        ax.grid()\n",
    "\n",
    "        ax.legend(['Equity curve', 'S&P 500 portflio'])\n",
    "\n",
    "        if show:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mwrapper / K fold cross validatin\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Number of jobs to run in parallel\n",
    "# Set to number of computer cores to use\n",
    "N_JOBS = 10\n",
    "N_SPLITS = 5\n",
    "N_REPEATS = 4\n",
    "\n",
    "def _fit_and_score(classifier, X, y, w, train_index, test_index, i) -> float:\n",
    "    \"\"\"\n",
    "    The function used by joblib to split, train, and score cross validations\n",
    "    \"\"\"\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "\n",
    "    w_train = w.iloc[train_index]\n",
    "    w_test = w.iloc[test_index]\n",
    "\n",
    "    classifier.fit(X_train, y_train, w_train)\n",
    "    score = classifier.score(X_test, y_test, w_test)\n",
    "\n",
    "    print(f'Finished {i} ({100*score:.1f}%)')\n",
    "\n",
    "    return score\n",
    "\n",
    "def repeated_k_fold(classifier, X, y, w) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform repeated k-fold cross validation on a classifier. Spread fitting \n",
    "    job over multiple computer cores.\n",
    "    \"\"\"\n",
    "    n_jobs = N_JOBS\n",
    "\n",
    "    n_splits = N_SPLITS\n",
    "    n_repeats = N_REPEATS\n",
    "\n",
    "    total_fits =  n_splits * n_repeats\n",
    "\n",
    "    _k_fold = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
    "\n",
    "    print(f'Fitting {total_fits} models {n_jobs} at a time ...')\n",
    "    print()\n",
    "\n",
    "    parallel = Parallel(n_jobs=n_jobs)\n",
    "    scores = parallel(\n",
    "        delayed(_fit_and_score)(\n",
    "            clone(classifier), X, y, w, train_index, test_index, i\n",
    "        ) for i, (train_index, test_index) in enumerate(_k_fold.split(X))\n",
    "    )\n",
    "\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "def calculate_model(df: pd.DataFrame) -> RandomForestClassifier:\n",
    "    \"\"\"\n",
    "    Given a dataframe with a y column, weights column, and predictor columns \n",
    "    with arbitrary names, cross-validated and fit a classifier. Print \n",
    "    diagnostics.\n",
    "    \"\"\"\n",
    "    classifier = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "    # Separate data\n",
    "    predictor_columns = [\n",
    "        c for c in df.columns.values if not c in ('y', 'weights')\n",
    "    ]\n",
    "    X = df[predictor_columns]\n",
    "    y = df['y']\n",
    "    w = df['weights']\n",
    "\n",
    "    # Fit cross validation\n",
    "    scores = repeated_k_fold(classifier, X, y, w)\n",
    "\n",
    "    # Get a full dataset fit for importance scores\n",
    "    classifier.fit(X, y, w)\n",
    "\n",
    "    # Compute diagnostics\n",
    "    _imp = classifier.feature_importances_\n",
    "    importance_series = pd.Series(_imp, index=predictor_columns)\n",
    "    importance_series = importance_series.sort_values(ascending=False)\n",
    "\n",
    "    # baseline accuracy is the best value achievable with a constant guess\n",
    "    baseline = np.max(y.value_counts() / y.shape[0])\n",
    "\n",
    "    # Compute a rough confidence interval for the improvement\n",
    "    mean_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "\n",
    "    upper_bound = mean_score + 2 * std_score\n",
    "    lower_bound = mean_score - 2 * std_score\n",
    "    ibounds = (lower_bound - baseline, upper_bound - baseline)\n",
    "\n",
    "    print()\n",
    "    print('Feature importances')\n",
    "    for col, imp in importance_series.items():\n",
    "        print(f'{col:24} {imp:>.3f}')\n",
    "    print()\n",
    "\n",
    "    print('Cross validation scores')\n",
    "    print(np.round(100 * scores, 1))\n",
    "    print()\n",
    "\n",
    "    print(f'Baseline accuracy {100*baseline:.1f}%')\n",
    "    print(f'OOS accuracy {100*mean_score:.1f}% +/- {200 * scores.std():.1f}%')\n",
    "    print(f'Improvement {100*(ibounds[0]):.1f} to {100*(ibounds[1]):.1f}%')\n",
    "    print()\n",
    "\n",
    "    return classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
